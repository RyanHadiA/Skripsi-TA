{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1WrWOCYZkLn6bNL_FIG9m4DhK5jJjzV0F","timestamp":1753448037906},{"file_id":"1wMTlQuSp9bdcWR0nYkdH_d0lI8YBNXoY","timestamp":1751172099121},{"file_id":"1DNc2DZoGgwCPKrj_ELKH6FOcGt759wt_","timestamp":1750428588976},{"file_id":"1ZiuLlo1n68epvwSKIJUIgW8TOjR6BTRO","timestamp":1750323783773}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Load & Split data"],"metadata":{"id":"UseLSK2BQ72w"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"-m0HSF7xq5jH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.utils import shuffle\n","import re\n","\n","file_path = '/content/drive/MyDrive/Combined All Dataset/Teks panjang-pendek(192)/preprocessed_Gabungan Seluruh Dataset_long6046.csv'\n","#file_path = '/content/drive/MyDrive/Combined All Dataset/Teks panjang-pendek(192)/preprocessed_Gabungan Seluruh Dataset_short31584.csv'\n","df = pd.read_csv(file_path)\n","\n","# Hitung total jumlah data\n","total_samples = len(df)\n","print(f\"Total data: {total_samples}\\n\")\n","\n","# ==== Split Dataset =====\n","# Mengacak dataset terlebih dahulu\n","df = shuffle(df, random_state=42)\n","\n","# Split: 80% train, 10% validation, 10% test\n","train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42)\n","val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n","\n","# Print jumlah tiap subset\n","print(f\"Jumlah data pelatihan: {len(train_df)}\")\n","print(f\"Jumlah data validasi: {len(val_df)}\")\n","print(f\"Jumlah data pengujian: {len(test_df)}\")"],"metadata":{"id":"Abt6V7CK73f3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Tokenizer BERT"],"metadata":{"id":"GOh15b4LUdyG"}},{"cell_type":"code","source":["from transformers import DistilBertTokenizerFast\n","import torch\n","from torch.utils.data import Dataset\n","from typing import Optional, Union\n","import pandas as pd\n","import os\n","import random\n","import numpy as np\n","\n","torch.manual_seed(42)\n","np.random.seed(42)\n","random.seed(42)\n","\n","\n","tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n","\n","\n","class ConfigTM :\n","    MAX_LENGTH = 256\n","    HIDDEN_DIM = 256\n","\n","\n","class PersonalityDataset(Dataset):\n","    def __init__(\n","        self,\n","        data: Union[str, pd.DataFrame],\n","        tokenizer: DistilBertTokenizerFast,\n","        max_len: int = ConfigTM.MAX_LENGTH,\n","        text_col: str = \"Text\",\n","        label_cols: Optional[list] = None\n","    ):\n","        if isinstance(data, str):\n","            if not os.path.isfile(data):\n","                raise FileNotFoundError(f\"File '{data}' tidak ditemukan.\")\n","            df = pd.read_csv(data)\n","        elif isinstance(data, pd.DataFrame):\n","            df = data.copy()\n","        else:\n","            raise ValueError(\"Argumen `data` harus str (path) atau pd.DataFrame.\")\n","\n","        if text_col not in df.columns:\n","            raise ValueError(f\"Kolom '{text_col}' tidak ditemukan di data.\")\n","        if label_cols is None:\n","            label_cols = df.columns[-5:].tolist()\n","        for lab in label_cols:\n","            if lab not in df.columns:\n","                raise ValueError(f\"Kolom label '{lab}' tidak ditemukan di data.\")\n","\n","        self.tokenizer = tokenizer\n","        self.max_len = max_len\n","\n","        self.texts = df[text_col].astype(str).tolist()\n","        self.labels = df[label_cols].astype(float).values\n","\n","        encodings = tokenizer(\n","            self.texts,\n","            add_special_tokens=True,\n","            max_length=max_len,\n","            padding='max_length',\n","            truncation=True,\n","            return_attention_mask=True,\n","            return_tensors='pt'\n","        )\n","\n","        self.input_ids = encodings['input_ids']\n","        self.attention_mask = encodings['attention_mask']\n","\n","        self.labels_tensor = torch.tensor(self.labels, dtype=torch.float)\n","\n","    def __len__(self) -> int:\n","        return len(self.texts)\n","\n","    def __getitem__(self, idx: int) -> dict:\n","        return {\n","            'input_ids': self.input_ids[idx],\n","            'attention_mask': self.attention_mask[idx],\n","            'labels': self.labels_tensor[idx]\n","        }\n","\n","\n","train_dataset = PersonalityDataset(train_df, tokenizer, max_len=ConfigTM.MAX_LENGTH)\n","val_dataset   = PersonalityDataset(val_df,   tokenizer, max_len=ConfigTM.MAX_LENGTH)\n","test_dataset  = PersonalityDataset(test_df,  tokenizer, max_len=ConfigTM.MAX_LENGTH)\n","\n","\n","print(\"\\nContoh hasil preprocessing 1 sample:\")\n","sample = train_dataset[0]\n","print(f\"Input IDs    : {sample['input_ids'].shape}  # tensor of length {sample['input_ids'].shape[0]}\")\n","print(f\"Attention Mask: {sample['attention_mask'].shape}\")\n","print(f\"Labels (OCEAN): {sample['labels']}  # shape {sample['labels'].shape}\")"],"metadata":{"id":"K2ZRBZSSUhVT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Definisi Model"],"metadata":{"id":"PHYuDEdQVD_I"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","from transformers import DistilBertModel\n","\n","class SimpleBertOcean(nn.Module):\n","    def __init__(\n","        self,\n","        pretrained_model_name: str = 'distilbert-base-uncased',\n","        hidden_dim: int = ConfigTM.HIDDEN_DIM,\n","        dropout: float = 0.3,\n","        num_labels: int = 5,\n","        freeze_bert: bool = True\n","    ):\n","        super(SimpleBertOcean, self).__init__()\n","\n","        self.bert = DistilBertModel.from_pretrained(pretrained_model_name)\n","\n","\n","        if freeze_bert:\n","            for param in self.bert.parameters():\n","                param.requires_grad = False\n","\n","\n","        self.classifier = nn.Sequential(\n","            nn.Linear(self.bert.config.hidden_size, hidden_dim),\n","            nn.ReLU(),\n","            nn.Dropout(dropout),\n","            nn.Linear(hidden_dim, num_labels)\n","\n","        )\n","\n","    def forward(self, input_ids, attention_mask):\n","\n","        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n","        last_hidden_states = outputs.last_hidden_state\n","        pooled = last_hidden_states[:, 0, :]\n","\n","\n","        logits = self.classifier(pooled)\n","        return logits\n","\n","model = SimpleBertOcean(\n","    pretrained_model_name='distilbert-base-uncased',\n","    hidden_dim=ConfigTM.HIDDEN_DIM,\n","    dropout=0.3,\n","    num_labels=5,\n","    freeze_bert=True\n",")\n","\n","print(model)"],"metadata":{"id":"Z4raSZGuVJIJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Hyperparameter dan pelatihan"],"metadata":{"id":"R1RrEU9nVO1W"}},{"cell_type":"code","source":["import time\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader\n","from torch.optim import AdamW, SGD\n","import matplotlib.pyplot as plt\n","from google.colab import drive\n","import numpy as np\n","import random\n","from transformers import DistilBertModel\n","\n","torch.manual_seed(42)\n","np.random.seed(42)\n","random.seed(42)\n","\n","class Config:\n","    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","    LEARNING_RATE = 5e-5\n","    BATCH_SIZE    = 32\n","    EPOCHS        = 3\n","\n","    OPTIMIZER = 'adamw'\n","\n","    LOSS_FN = 'bce_logits'\n","\n","    PIN_MEMORY  = True\n","    NUM_WORKERS = 2\n","\n","    CHECKPOINT_PATH = f'/content/drive/MyDrive/model_checkpoints/v1-tambahan metode threshold/distilbert_cekpt_Gabungan Seluruh Dataset_biner([{LEARNING_RATE}]-{BATCH_SIZE}-{EPOCHS}-{NUM_WORKERS})MX{ConfigTM.MAX_LENGTH} H{ConfigTM.HIDDEN_DIM} JMLH{len(df)}.pth'\n","\n","train_loader = DataLoader(\n","    train_dataset,\n","    batch_size=Config.BATCH_SIZE,\n","    shuffle=True,\n","    pin_memory=Config.PIN_MEMORY,\n","    num_workers=Config.NUM_WORKERS\n",")\n","val_loader = DataLoader(\n","    val_dataset,\n","    batch_size=Config.BATCH_SIZE,\n","    shuffle=False,\n","    pin_memory=Config.PIN_MEMORY,\n","    num_workers=Config.NUM_WORKERS\n",")\n","\n","model = SimpleBertOcean(\n","    pretrained_model_name='distilbert-base-uncased',\n","    hidden_dim=ConfigTM.HIDDEN_DIM,\n","    dropout=0.3,\n","    num_labels=5,\n","    freeze_bert=True\n",").to(Config.DEVICE)\n","\n","\n","if Config.LOSS_FN.lower() == 'bce':\n","    loss_fn = nn.BCELoss()\n","else:\n","    loss_fn = nn.BCEWithLogitsLoss()\n","\n","\n","trainable_params = filter(lambda p: p.requires_grad, model.parameters())\n","if Config.OPTIMIZER.lower() == 'sgd':\n","    optimizer = SGD(trainable_params, lr=Config.LEARNING_RATE)\n","else:\n","    optimizer = AdamW(trainable_params, lr=Config.LEARNING_RATE)\n","\n","\n","\n","def train_epoch(loader, model, loss_fn, optimizer, device):\n","    model.train()\n","    running_loss = 0.0\n","    for batch in loader:\n","        input_ids      = batch['input_ids'].to(device)\n","        attention_mask = batch['attention_mask'].to(device)\n","        labels         = batch['labels'].to(device)\n","\n","        logits = model(input_ids=input_ids, attention_mask=attention_mask)\n","        loss   = loss_fn(logits, labels)\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item()\n","\n","    return running_loss / len(loader)\n","\n","def eval_epoch(loader, model, loss_fn, device):\n","    model.eval()\n","    running_loss = 0.0\n","    with torch.no_grad():\n","        for batch in loader:\n","            input_ids      = batch['input_ids'].to(device)\n","            attention_mask = batch['attention_mask'].to(device)\n","            labels         = batch['labels'].to(device)\n","\n","            logits = model(input_ids=input_ids, attention_mask=attention_mask)\n","            loss   = loss_fn(logits, labels)\n","            running_loss += loss.item()\n","\n","    return running_loss / len(loader)\n","\n","\n","train_losses, val_losses = [], []\n","start_all = time.time()\n","\n","for epoch in range(1, Config.EPOCHS + 1):\n","    train_loss = train_epoch(train_loader, model, loss_fn, optimizer, Config.DEVICE)\n","    val_loss   = eval_epoch(val_loader,   model, loss_fn, Config.DEVICE)\n","\n","    train_losses.append(train_loss)\n","    val_losses.append(val_loss)\n","\n","    print(f\"Epoch {epoch}/{Config.EPOCHS} | \"\n","          f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n","\n","total_secs     = time.time() - start_all\n","total_time_str = time.strftime(\"%H:%M:%S\", time.gmtime(total_secs))\n","print(f\"\\nTotal waktu pelatihan: {total_time_str}\\n\")\n","\n","\n","plt.figure(figsize=(8,5))\n","plt.plot(range(1, Config.EPOCHS+1), train_losses, label='Train Loss', marker='o')\n","plt.plot(range(1, Config.EPOCHS+1), val_losses,   label='Val Loss',   marker='s')\n","plt.xlabel('Epoch'); plt.ylabel('Loss')\n","plt.title('Loss Curve (Train vs Val)')\n","plt.legend(); plt.grid(alpha=0.3)\n","plt.show()"],"metadata":{"id":"eQtbyjMTVSlb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Simpan pelatihan sebelum ke tahap selanjutnya"],"metadata":{"id":"M2NtYVbwe4jK"}},{"cell_type":"code","metadata":{"id":"08dde0d0"},"source":["import torch\n","import os\n","\n","os.makedirs(os.path.dirname(Config.CHECKPOINT_PATH), exist_ok=True)\n","\n","checkpoint = {\n","    'dataset_info': {\n","        'total_data': len(df),\n","        'train_size': len(train_dataset),\n","        'val_size'  : len(val_dataset),\n","        'test_size' : len(test_dataset)\n","    },\n","    'model_state_dict': model.state_dict(),\n","    'optimizer_state_dict': optimizer.state_dict(),\n","    'hyperparameters': {\n","        'learning_rate': Config.LEARNING_RATE,\n","        'batch_size'   : Config.BATCH_SIZE,\n","        'epochs'       : Config.EPOCHS,\n","        'optimizer'    : Config.OPTIMIZER,\n","        'loss_fn'      : Config.LOSS_FN,\n","        'pin_memory'   : Config.PIN_MEMORY,\n","        'num_workers'  : Config.NUM_WORKERS,\n","        'max_length'   : ConfigTM.MAX_LENGTH,\n","        'freeze_bert'  : True,\n","        'pretrained_model_name': 'distilbert-base-uncased',\n","        'hidden_dim'   : ConfigTM.HIDDEN_DIM,\n","        'dropout'      : 0.3,\n","        'num_labels'   : 5\n","    },\n","    'train_losses'  : train_losses,\n","    'val_losses'    : val_losses,\n","    'training_time' : total_time_str\n","}\n","\n","torch.save(checkpoint, Config.CHECKPOINT_PATH)\n","print(f\"Checkpoint tersimpan di: {Config.CHECKPOINT_PATH}\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Metode Threshold Tuning"],"metadata":{"id":"RIIl7-v1ea7Y"}},{"cell_type":"code","metadata":{"id":"2a568615"},"source":["import numpy as np\n","from sklearn.metrics import precision_recall_fscore_support, matthews_corrcoef\n","import torch\n","from torch.utils.data import DataLoader\n","\n","def find_optimal_thresholds_combined(\n","    probs,\n","    labels,\n","    num_labels=5,\n","    step=0.05,\n","    alpha=0.5,\n","    label_names=None\n","):\n","    \"\"\"\n","    Mencari threshold optimal per dimensi dengan mengoptimalkan kombinasi F1 dan MCC.\n","    - alpha: bobot untuk F1 (0.0 = hanya MCC, 1.0 = hanya F1).\n","    \"\"\"\n","    if label_names is None:\n","        label_names = [f\"Dimensi_{i}\" for i in range(num_labels)]\n","\n","    optimal_thresholds = []\n","    print(\"Mencari threshold optimal per dimensi (mengoptimalkan kombinasi F1 & MCC)...\")\n","    for i in range(num_labels):\n","        best_score    = -1\n","        best_f1       = 0\n","        best_mcc      = -1\n","        best_thresh   = 0.5\n","\n","        for thresh in np.arange(0.05, 1.0, step):\n","            preds_binary = (probs[:, i] > thresh).astype(int)\n","\n","            if len(np.unique(preds_binary)) < 2:\n","                continue\n","\n","            _, _, f1, _ = precision_recall_fscore_support(\n","                labels[:, i], preds_binary, average='binary', zero_division=0\n","            )\n","            mcc = matthews_corrcoef(labels[:, i], preds_binary)\n","\n","            mcc_norm = (mcc + 1) / 2.0\n","\n","\n","            score = alpha * f1 + (1 - alpha) * mcc_norm\n","\n","            if score > best_score:\n","                best_score  = score\n","                best_f1     = f1\n","                best_mcc    = mcc\n","                best_thresh = thresh\n","\n","        optimal_thresholds.append(best_thresh)\n","        print(\n","            f\"  {label_names[i]} | \"\n","            f\"Threshold={best_thresh:.4f} | \"\n","            f\"F1={best_f1:.4f} | \"\n","            f\"MCC={best_mcc:.4f} | \"\n","            f\"Score_combined={best_score:.4f}\"\n","        )\n","\n","    return optimal_thresholds\n","\n","\n","def get_preds_labels(loader, model, device):\n","    \"\"\"\n","    Mengumpulkan prediksi (probabilities) dan label asli dari DataLoader.\n","    \"\"\"\n","    model.eval()\n","    all_probs, all_labels = [], []\n","    with torch.no_grad():\n","        for batch in loader:\n","            input_ids      = batch['input_ids'].to(device)\n","            attention_mask = batch['attention_mask'].to(device)\n","            labels         = batch['labels'].cpu().numpy()\n","\n","            logits = model(input_ids=input_ids, attention_mask=attention_mask)\n","\n","            probs = torch.sigmoid(logits).cpu().numpy()\n","\n","            all_probs.append(probs)\n","            all_labels.append(labels)\n","\n","\n","    preds_np  = np.vstack(all_probs)\n","    labels_np = np.vstack(all_labels)\n","\n","    return preds_np, labels_np\n","\n","val_probs, val_labels = get_preds_labels(val_loader, model, Config.DEVICE)\n","\n","label_names = ['O', 'C', 'E', 'A', 'N']\n","\n","\n","val_thresholds = find_optimal_thresholds_combined(\n","    val_probs, val_labels,\n","    label_names=label_names,\n","    alpha=0.5,\n","    step=0.001\n",")\n","\n","print(\"\\nOptimal thresholds per dimensi:\")\n","for ln, t in zip(label_names, val_thresholds):\n","    print(f\" - {ln}: {t:.4f}\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Tes & Evaluasi"],"metadata":{"id":"v4M2uumqV5uV"}},{"cell_type":"code","source":["import numpy as np\n","from sklearn.metrics import (\n","    accuracy_score,\n","    precision_recall_fscore_support,\n","    matthews_corrcoef\n",")\n","from torch.utils.data import DataLoader\n","import torch\n","import random\n","\n","torch.manual_seed(42)\n","np.random.seed(42)\n","random.seed(42)\n","\n","\n","test_loader = DataLoader(\n","    test_dataset,\n","    batch_size=Config.BATCH_SIZE,\n","    shuffle=False,\n","    pin_memory=Config.PIN_MEMORY,\n","    num_workers=Config.NUM_WORKERS\n",")\n","\n","\n","def test_and_evaluate_per_dimension(test_loader, model, device, thresholds, label_names=None):\n","    \"\"\"\n","    Menghitung metrik overall (micro) dan per-dimensi (O, C, E, A, N)\n","    menggunakan threshold yang diberikan.\n","    \"\"\"\n","    model.eval()\n","    all_probs, all_labels = [], []\n","\n","    with torch.no_grad():\n","        for batch in test_loader:\n","            input_ids      = batch['input_ids'].to(device)\n","            attention_mask = batch['attention_mask'].to(device)\n","            labels         = batch['labels'].cpu().numpy()\n","\n","            logits = model(input_ids=input_ids, attention_mask=attention_mask)\n","            probs = torch.sigmoid(logits).cpu().numpy()\n","\n","            all_probs.append(probs)\n","            all_labels.append(labels)\n","\n","\n","    preds  = np.vstack(all_probs)\n","    labels = np.vstack(all_labels)\n","\n","\n","    preds_binary = np.zeros_like(preds, dtype=int)\n","    for i in range(preds.shape[1]):\n","        preds_binary[:, i] = (preds[:, i] > thresholds[i]).astype(int)\n","\n","\n","    num_labels = labels.shape[1]\n","\n","\n","    overall_micro_acc = accuracy_score(labels.flatten(), preds_binary.flatten())\n","\n","    precision_micro, recall_micro, f1_micro, _ = precision_recall_fscore_support(\n","        labels.flatten(), preds_binary.flatten(), average='binary', zero_division=0\n","    )\n","\n","    mcc_overall = matthews_corrcoef(labels.flatten(), preds_binary.flatten())\n","\n","\n","\n","    dim_acc = {i: accuracy_score(labels[:, i], preds_binary[:, i]) for i in range(num_labels)}\n","\n","    precision_per_label = []\n","    recall_per_label = []\n","    f1_per_label = []\n","    mcc_per_label = {}\n","\n","    for i in range(num_labels):\n","\n","        if len(np.unique(preds_binary[:, i])) < 2 or len(np.unique(labels[:, i])) < 2:\n","             p, r, f, _ = 0.0, 0.0, 0.0, None\n","             mcc = 0.0\n","        else:\n","            p, r, f, _ = precision_recall_fscore_support(\n","                labels[:, i], preds_binary[:, i], average='binary', zero_division=0\n","            )\n","            mcc = matthews_corrcoef(labels[:, i], preds_binary[:, i])\n","\n","        precision_per_label.append(p)\n","        recall_per_label.append(r)\n","        f1_per_label.append(f)\n","        mcc_per_label[i] = mcc\n","\n","\n","    print(\"---- Test Metrics overall (with Optimized Thresholds) ----\")\n","    print(f\"Akurasi   : {overall_micro_acc:.4f}\")\n","    print(f\"Presisi   : {precision_micro:.4f}\")\n","    print(f\"Recall    : {recall_micro:.4f}\")\n","    print(f\"F1-score  : {f1_micro:.4f}\")\n","    print(f\"MCC       : {mcc_overall:.4f}\\n\")\n","\n","\n","    if label_names is None:\n","        label_names = [f\"Dimensi_{i}\" for i in range(num_labels)]\n","\n","    print(\"---- Akurasi, Precision, Recall, F1, MCC Tiap Dimensi ----\")\n","    print(f\"{'Label':<8} | {'Accuracy':>8} | {'Precision':>9} | {'Recall':>7} | {'F1-score':>8} | {'MCC':>7} | {'Threshold':>9}\")\n","    print(\"-\" * 85)\n","    for i, name in enumerate(label_names):\n","        print(\n","            f\"{name:<8} | \"\n","            f\"{dim_acc[i]:>8.4f} | \"\n","            f\"{precision_per_label[i]:>9.4f} | \"\n","            f\"{recall_per_label[i]:>7.4f} | \"\n","            f\"{f1_per_label[i]:>8.4f} | \"\n","            f\"{mcc_per_label[i]:>7.4f} | \"\n","            f\"{thresholds[i]:>9.4f}\"\n","        )\n","\n","\n","    best_acc_idx = max(dim_acc, key=lambda x: dim_acc[x])\n","    best_f1_idx  = int(np.argmax(f1_per_label))\n","    best_mcc_idx = max(mcc_per_label, key=lambda x: mcc_per_label[x])\n","\n","\n","    win_counts = {i: 0 for i in range(num_labels)}\n","\n","    if dim_acc[best_acc_idx] > 0: win_counts[best_acc_idx] += 1\n","    if f1_per_label[best_f1_idx] > 0: win_counts[best_f1_idx] += 1\n","    if mcc_per_label[best_mcc_idx] > -1: win_counts[best_mcc_idx] += 1\n","\n","\n","    if max(win_counts.values()) == 0:\n","         best_dim_idx = int(np.argmax(f1_per_label))\n","    else:\n","         best_dim_idx = max(win_counts, key=lambda x: win_counts[x])\n","\n","    best_dim_name = label_names[best_dim_idx]\n","\n","\n","    best_acc_val = dim_acc[best_dim_idx]\n","    best_f1_val  = f1_per_label[best_dim_idx]\n","    best_mcc_val = mcc_per_label[best_dim_idx]\n","    best_thresh_val = thresholds[best_dim_idx]\n","\n","\n","\n","    print(\n","        f\"\\nDimensi terbaik diprediksi (berdasarkan voting Accuracy, F1, MCC): \"\n","        f\"( {best_dim_name} ) \"\n","        f\"(Akurasi = {best_acc_val:.4f}) \"\n","        f\"(F1 = {best_f1_val:.4f}) \"\n","        f\"(MCC = {best_mcc_val:.4f}) \"\n","        f\"(Threshold = {best_thresh_val:.4f})\"\n","    )\n","\n","\n","    metrics = {\n","        'overall_micro_acc': overall_micro_acc,\n","        'precision_micro': precision_micro,\n","        'recall_micro': recall_micro,\n","        'f1_micro': f1_micro,\n","        'mcc_overall': mcc_overall,\n","        'accuracy_per_dim': {label_names[i]: dim_acc[i] for i in range(num_labels)},\n","        'precision_per_dim': {label_names[i]: precision_per_label[i] for i in range(num_labels)},\n","        'recall_per_dim'   : {label_names[i]: recall_per_label[i] for i in range(num_labels)},\n","        'f1_per_dim'       : {label_names[i]: f1_per_label[i] for i in range(num_labels)},\n","        'mcc_per_dim'      : {label_names[i]: mcc_per_label[i] for i in range(num_labels)},\n","        'optimal_thresholds': {label_names[i]: thresholds[i] for i in range(num_labels)},\n","\n","        'best_dim_by_vote': {\n","            'name': best_dim_name,\n","            'accuracy': best_acc_val,\n","            'f1': best_f1_val,\n","            'mcc': best_mcc_val,\n","            'threshold': best_thresh_val,\n","            'votes': win_counts[best_dim_idx]\n","        }\n","    }\n","    return metrics\n","\n","\n","label_names = ['O', 'C', 'E', 'A', 'N']\n","\n","test_metrics = test_and_evaluate_per_dimension(test_loader, model, Config.DEVICE, val_thresholds, label_names)"],"metadata":{"id":"l8vmbI4OV9LZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Simpan Hasil"],"metadata":{"id":"8I8aCe5UWAQ_"}},{"cell_type":"markdown","source":["Simpan Metadata"],"metadata":{"id":"MnnIrng3B3VV"}},{"cell_type":"code","source":["import json\n","import pandas as pd\n","from google.colab import files\n","import os\n","\n","metadata = {\n","    'dataset_info': {\n","        'total_data': len(df),\n","        'train_size': len(train_dataset),\n","        'val_size':   len(val_dataset),\n","        'test_size':  len(test_dataset)\n","    },\n","    'hyperparameters': {\n","        'learning_rate': Config.LEARNING_RATE,\n","        'batch_size'   : Config.BATCH_SIZE,\n","        'epochs'       : Config.EPOCHS,\n","        'optimizer'    : Config.OPTIMIZER,\n","        'loss_fn'      : Config.LOSS_FN,\n","        'pin_memory'   : Config.PIN_MEMORY,\n","        'num_workers'  : Config.NUM_WORKERS,\n","        'max_length'   : ConfigTM.MAX_LENGTH,\n","        'freeze_bert'  : True,\n","        'pretrained_model_name': 'distilbert-base-uncased',\n","        'hidden_dim'   : ConfigTM.HIDDEN_DIM,\n","        'dropout'      : 0.3,\n","        'num_labels'   : 5\n","    },\n","    'train_losses': train_losses,\n","    'val_losses':   val_losses,\n","    'val_thresholds': val_thresholds,\n","    'test_metrics': test_metrics,\n","    'training_time': total_time_str\n","}\n","\n","\n","metadata_path = os.path.join(os.path.dirname(Config.CHECKPOINT_PATH), f'distilbert_metadata_threshold_tuned_Gabungan Seluruh Dataset_biner([{Config.LEARNING_RATE}]-{Config.BATCH_SIZE}-{Config.EPOCHS}-{Config.NUM_WORKERS})MX{ConfigTM.MAX_LENGTH} H{ConfigTM.HIDDEN_DIM} JMLH{len(df)}.json')\n","\n","with open(metadata_path, 'w') as f:\n","    json.dump(metadata, f, indent=2)\n","\n","print(f\"Metadata saved to {metadata_path}\")\n","\n","try:\n","    files.download(metadata_path)\n","except Exception as e:\n","    print(f\"Could not automatically download the file. Please download it manually from: {metadata_path}\")"],"metadata":{"id":"_KgyK4cHB6wU"},"execution_count":null,"outputs":[]}]}