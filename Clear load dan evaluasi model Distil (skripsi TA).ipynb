{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1RaPRFvBvO2KKTFng8e7St2DPcuCXYsyO","timestamp":1753448448874}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"IQEQpsSR78QM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.utils import shuffle\n","import re\n","\n","file_path = '/content/drive/MyDrive/Combined All Dataset/Teks panjang-pendek(192)/preprocessed_Gabungan Seluruh Dataset_long6046.csv'\n","#file_path = '/content/drive/MyDrive/Combined All Dataset/Teks panjang-pendek(192)/preprocessed_Gabungan Seluruh Dataset_short31584.csv'\n","df = pd.read_csv(file_path)\n","\n","# Hitung total jumlah data\n","total_samples = len(df)\n","print(f\"Total data: {total_samples}\\n\")\n","\n","# ==== Split Dataset =====\n","# Mengacak dataset terlebih dahulu\n","df = shuffle(df, random_state=42)\n","\n","# Split: 80% train, 10% validation, 10% test\n","train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42)\n","val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n","\n","# Print jumlah tiap subset\n","print(f\"Jumlah data pelatihan: {len(train_df)}\")\n","print(f\"Jumlah data validasi: {len(val_df)}\")\n","print(f\"Jumlah data pengujian: {len(test_df)}\")"],"metadata":{"id":"7acM8msDnnou"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Tokenizer"],"metadata":{"id":"W23arRavrTUA"}},{"cell_type":"code","source":["from transformers import DistilBertTokenizerFast\n","import torch\n","from torch.utils.data import Dataset\n","from typing import Optional, Union\n","import pandas as pd\n","import os\n","import random\n","import numpy as np\n","\n","torch.manual_seed(42)\n","np.random.seed(42)\n","random.seed(42)\n","\n","tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n","\n","class ConfigTM :\n","    MAX_LENGTH = 256\n","    HIDDEN_DIM = 256\n","\n","class PersonalityDataset(Dataset):\n","    def __init__(\n","        self,\n","        data: Union[str, pd.DataFrame],\n","        tokenizer: DistilBertTokenizerFast,\n","        max_len: int = ConfigTM.MAX_LENGTH,\n","        text_col: str = \"Text\",\n","        label_cols: Optional[list] = None\n","    ):\n","\n","        if isinstance(data, str):\n","            if not os.path.isfile(data):\n","                raise FileNotFoundError(f\"File '{data}' tidak ditemukan.\")\n","            df = pd.read_csv(data)\n","        elif isinstance(data, pd.DataFrame):\n","            df = data.copy()\n","        else:\n","            raise ValueError(\"Argumen `data` harus str (path) atau pd.DataFrame.\")\n","\n","        if text_col not in df.columns:\n","            raise ValueError(f\"Kolom '{text_col}' tidak ditemukan di data.\")\n","        if label_cols is None:\n","            label_cols = df.columns[-5:].tolist()\n","        for lab in label_cols:\n","            if lab not in df.columns:\n","                raise ValueError(f\"Kolom label '{lab}' tidak ditemukan di data.\")\n","\n","        self.tokenizer = tokenizer\n","        self.max_len = max_len\n","\n","        self.texts = df[text_col].astype(str).tolist()\n","        self.labels = df[label_cols].astype(float).values\n","\n","        encodings = tokenizer(\n","            self.texts,\n","            add_special_tokens=True,\n","            max_length=max_len,\n","            padding='max_length',\n","            truncation=True,\n","            return_attention_mask=True,\n","            return_tensors='pt'\n","        )\n","        self.input_ids = encodings['input_ids']\n","        self.attention_mask = encodings['attention_mask']\n","        self.labels_tensor = torch.tensor(self.labels, dtype=torch.float)\n","\n","    def __len__(self) -> int:\n","        return len(self.texts)\n","\n","    def __getitem__(self, idx: int) -> dict:\n","        return {\n","            'input_ids': self.input_ids[idx],\n","            'attention_mask': self.attention_mask[idx],\n","            'labels': self.labels_tensor[idx]\n","        }\n","\n","\n","train_dataset = PersonalityDataset(train_df, tokenizer, max_len=ConfigTM.MAX_LENGTH)\n","val_dataset   = PersonalityDataset(val_df,   tokenizer, max_len=ConfigTM.MAX_LENGTH)\n","test_dataset  = PersonalityDataset(test_df,  tokenizer, max_len=ConfigTM.MAX_LENGTH)\n","\n","print(\"\\nContoh hasil preprocessing 1 sample:\")\n","sample = train_dataset[0]\n","print(f\"Input IDs    : {sample['input_ids'].shape}  # tensor of length {sample['input_ids'].shape[0]}\")\n","print(f\"Attention Mask: {sample['attention_mask'].shape}\")\n","print(f\"Labels (OCEAN): {sample['labels']}  # shape {sample['labels'].shape}\")"],"metadata":{"id":"vmRsHRbGnv_2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","from transformers import DistilBertModel\n","\n","class SimpleBertOcean(nn.Module):\n","    def __init__(\n","        self,\n","        pretrained_model_name: str = 'distilbert-base-uncased',\n","        hidden_dim: int = ConfigTM.HIDDEN_DIM,\n","        dropout: float = 0.3,\n","        num_labels: int = 5,\n","        freeze_bert: bool = True\n","    ):\n","        super(SimpleBertOcean, self).__init__()\n","        self.bert = DistilBertModel.from_pretrained(pretrained_model_name)\n","\n","        if freeze_bert:\n","            for param in self.bert.parameters():\n","                param.requires_grad = False\n","\n","        self.classifier = nn.Sequential(\n","            nn.Linear(self.bert.config.hidden_size, hidden_dim),\n","            nn.ReLU(),\n","            nn.Dropout(dropout),\n","            nn.Linear(hidden_dim, num_labels)\n","        )\n","\n","    def forward(self, input_ids, attention_mask):\n","        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n","        last_hidden_states = outputs.last_hidden_state\n","        pooled = last_hidden_states[:, 0, :]\n","\n","        logits = self.classifier(pooled)\n","        return logits\n","\n","model = SimpleBertOcean(\n","    pretrained_model_name='distilbert-base-uncased',\n","    hidden_dim=ConfigTM.HIDDEN_DIM,\n","    dropout=0.3,\n","    num_labels=5,\n","    freeze_bert=True\n",")\n","\n","print(model)"],"metadata":{"id":"7n5s6_clhfNk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import time\n","import torch\n","import torch.nn as nn\n","import numpy as np\n","import json\n","from sklearn.metrics import f1_score\n","from transformers import BertTokenizerFast, BertModel\n","from torch.utils.data import DataLoader\n","from torch.optim import AdamW, SGD\n","import matplotlib.pyplot as plt\n","import os\n","\n","torch.manual_seed(42)\n","np.random.seed(42)\n","random.seed(42)\n","\n","class Config:\n","    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    CHECKPOINT_PATH = '/content/drive/MyDrive/model_checkpoints/v1-tambahan metode threshold/Distil panjang/distilbert_cekpt_Gabungan Seluruh Dataset_biner([2e-05]-32-3-2)MX256 H256 JMLH6046.pth'\n","\n","\n","checkpoint = torch.load(Config.CHECKPOINT_PATH, map_location=Config.DEVICE)\n","\n","model = SimpleBertOcean(\n","    pretrained_model_name=checkpoint['hyperparameters']['pretrained_model_name'],\n","    hidden_dim=checkpoint['hyperparameters']['hidden_dim'],\n","    dropout=checkpoint['hyperparameters']['dropout'],\n","    num_labels=checkpoint['hyperparameters']['num_labels'],\n","    freeze_bert=checkpoint['hyperparameters']['freeze_bert']\n",").to(Config.DEVICE)\n","model.load_state_dict(checkpoint['model_state_dict'])\n","model.eval()\n","\n","def display_checkpoint_info(checkpoint):\n","    print(\"=== Dataset Info ===\")\n","    for k, v in checkpoint['dataset_info'].items():\n","        print(f\"{k}: {v}\")\n","\n","    print(\"\\n=== Hyperparameters ===\")\n","    for k, v in checkpoint['hyperparameters'].items():\n","        print(f\"{k}: {v}\")\n","\n","    print(\"\\n=== Training Metrics ===\")\n","    print(f\"Total Epochs: {len(checkpoint['train_losses'])}\")\n","    print(f\"Train Losses: {checkpoint['train_losses']}\")\n","    print(f\"Validation Losses: {checkpoint['val_losses']}\")\n","    print(f\"Training Time: {checkpoint['training_time']}\")\n","\n","display_checkpoint_info(checkpoint)\n","\n","val_loader = DataLoader(\n","    val_dataset,\n","    batch_size= checkpoint['hyperparameters'].get('batch_size'),\n","    shuffle=False,\n","    pin_memory= checkpoint['hyperparameters'].get('pin_memory'),\n","    num_workers= checkpoint['hyperparameters'].get('num_workers')\n",")"],"metadata":{"id":"u0PehSNYZPad"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","from sklearn.metrics import precision_recall_fscore_support, matthews_corrcoef\n","import torch\n","from torch.utils.data import DataLoader\n","\n","def find_optimal_thresholds_combined(\n","    probs,\n","    labels,\n","    num_labels=5,\n","    step=0.05,\n","    alpha=0.5,\n","    label_names=None\n","):\n","    \"\"\"\n","    Mencari threshold optimal per dimensi dengan mengoptimalkan kombinasi F1 dan MCC.\n","    - alpha: bobot untuk F1 (0.0 = hanya MCC, 1.0 = hanya F1).\n","    \"\"\"\n","    if label_names is None:\n","        label_names = [f\"Dimensi_{i}\" for i in range(num_labels)]\n","\n","    optimal_thresholds = []\n","    print(\"Mencari threshold optimal per dimensi (mengoptimalkan kombinasi F1 & MCC)...\")\n","    for i in range(num_labels):\n","        best_score  = -1\n","        best_f1     = 0\n","        best_mcc    = -1\n","        best_thresh = 0.5\n","\n","        for thresh in np.arange(0, 1.0, step):\n","            preds_binary = (probs[:, i] > thresh).astype(int)\n","            if len(np.unique(preds_binary)) < 2:\n","                continue\n","\n","            _, _, f1, _ = precision_recall_fscore_support(\n","                labels[:, i], preds_binary, average='binary', zero_division=0\n","            )\n","            mcc = matthews_corrcoef(labels[:, i], preds_binary)\n","            mcc_norm = (mcc + 1) / 2.0\n","\n","            score = alpha * f1 + (1 - alpha) * mcc_norm\n","\n","            if score > best_score:\n","                best_score  = score\n","                best_f1     = f1\n","                best_mcc    = mcc\n","                best_thresh = thresh\n","\n","        optimal_thresholds.append(best_thresh)\n","        print(\n","            f\"  {label_names[i]} | \"\n","            f\"Threshold={best_thresh:.4f} | \"\n","            f\"F1={best_f1:.4f} | \"\n","            f\"MCC={best_mcc:.4f} | \"\n","            f\"Score_combined={best_score:.4f}\"\n","        )\n","\n","    return optimal_thresholds\n","\n","def get_preds_labels(loader, model, device):\n","    \"\"\"\n","    Mengumpulkan prediksi (probabilities) dan label asli dari DataLoader.\n","    \"\"\"\n","    model.eval()\n","    all_probs, all_labels = [], []\n","    with torch.no_grad():\n","        for batch in loader:\n","            input_ids      = batch['input_ids'].to(device)\n","            attention_mask = batch['attention_mask'].to(device)\n","            labels         = batch['labels'].cpu().numpy()\n","\n","            logits = model(input_ids=input_ids, attention_mask=attention_mask)\n","            probs = torch.sigmoid(logits).cpu().numpy()\n","\n","            all_probs.append(probs)\n","            all_labels.append(labels)\n","\n","    preds_np  = np.vstack(all_probs)\n","    labels_np = np.vstack(all_labels)\n","\n","    return preds_np, labels_np\n","\n","val_probs, val_labels = get_preds_labels(val_loader, model, Config.DEVICE)\n","\n","label_names = ['O', 'C', 'E', 'A', 'N']\n","\n","val_thresholds = find_optimal_thresholds_combined(\n","    val_probs, val_labels,\n","    label_names=label_names,\n","    alpha=0.5,\n","    step=0.0001\n",")\n","\n","print(\"\\nOptimal thresholds per dimensi:\")\n","for ln, t in zip(label_names, val_thresholds):\n","    print(f\" - {ln}: {t:.4f}\")"],"metadata":{"id":"NXXUciowrF0I"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# evaluasi"],"metadata":{"id":"G0GplPzAa0NQ"}},{"cell_type":"code","source":["import numpy as np\n","from sklearn.metrics import (\n","    accuracy_score,\n","    precision_recall_fscore_support,\n","    matthews_corrcoef\n",")\n","from torch.utils.data import DataLoader\n","import torch\n","import random\n","\n","torch.manual_seed(42)\n","np.random.seed(42)\n","random.seed(42)\n","\n","test_loader = DataLoader(\n","    test_dataset,\n","    batch_size= checkpoint['hyperparameters'].get('batch_size'),\n","    shuffle=False,\n","    pin_memory= checkpoint['hyperparameters'].get('pin_memory'),\n","    num_workers= checkpoint['hyperparameters'].get('num_workers')\n",")\n","\n","def test_and_evaluate_per_dimension(test_loader, model, device, thresholds, label_names=None):\n","    \"\"\"\n","    Menghitung metrik overall (micro) dan per-dimensi (O, C, E, A, N)\n","    menggunakan threshold yang diberikan.\n","    \"\"\"\n","    model.eval()\n","    all_probs, all_labels = [], []\n","\n","    with torch.no_grad():\n","        for batch in test_loader:\n","            input_ids      = batch['input_ids'].to(device)\n","            attention_mask = batch['attention_mask'].to(device)\n","            labels         = batch['labels'].cpu().numpy()\n","\n","            logits = model(input_ids=input_ids, attention_mask=attention_mask)\n","            probs = torch.sigmoid(logits).cpu().numpy()\n","\n","            all_probs.append(probs)\n","            all_labels.append(labels)\n","\n","    preds  = np.vstack(all_probs)\n","    labels = np.vstack(all_labels)\n","\n","    preds_binary = np.zeros_like(preds, dtype=int)\n","    for i in range(preds.shape[1]):\n","        preds_binary[:, i] = (preds[:, i] > thresholds[i]).astype(int)\n","\n","\n","    num_labels = labels.shape[1]\n","\n","    overall_micro_acc = accuracy_score(labels.flatten(), preds_binary.flatten())\n","    precision_micro, recall_micro, f1_micro, _ = precision_recall_fscore_support(\n","        labels.flatten(), preds_binary.flatten(), average='binary', zero_division=0\n","    )\n","    mcc_overall = matthews_corrcoef(labels.flatten(), preds_binary.flatten())\n","\n","\n","    dim_acc = {i: accuracy_score(labels[:, i], preds_binary[:, i]) for i in range(num_labels)}\n","    precision_per_label = []\n","    recall_per_label = []\n","    f1_per_label = []\n","    mcc_per_label = {}\n","\n","    for i in range(num_labels):\n","        if len(np.unique(preds_binary[:, i])) < 2 or len(np.unique(labels[:, i])) < 2:\n","             p, r, f, _ = 0.0, 0.0, 0.0, None\n","             mcc = 0.0\n","        else:\n","            p, r, f, _ = precision_recall_fscore_support(\n","                labels[:, i], preds_binary[:, i], average='binary', zero_division=0\n","            )\n","            mcc = matthews_corrcoef(labels[:, i], preds_binary[:, i])\n","\n","        precision_per_label.append(p)\n","        recall_per_label.append(r)\n","        f1_per_label.append(f)\n","        mcc_per_label[i] = mcc\n","\n","\n","    print(\"---- Test Metrics overall (with Optimized Thresholds) ----\")\n","    print(f\"Akurasi   : {overall_micro_acc:.4f}\")\n","    print(f\"Presisi   : {precision_micro:.4f}\")\n","    print(f\"Recall    : {recall_micro:.4f}\")\n","    print(f\"F1-score  : {f1_micro:.4f}\")\n","    print(f\"MCC       : {mcc_overall:.4f}\\n\")\n","\n","    if label_names is None:\n","        label_names = [f\"Dimensi_{i}\" for i in range(num_labels)]\n","\n","    print(\"---- Akurasi, Precision, Recall, F1, MCC Tiap Dimensi ----\")\n","    print(f\"{'Label':<8} | {'Accuracy':>8} | {'Precision':>9} | {'Recall':>7} | {'F1-score':>8} | {'MCC':>7} | {'Threshold':>9}\")\n","    print(\"-\" * 85)\n","    for i, name in enumerate(label_names):\n","        print(\n","            f\"{name:<8} | \"\n","            f\"{dim_acc[i]:>8.4f} | \"\n","            f\"{precision_per_label[i]:>9.4f} | \"\n","            f\"{recall_per_label[i]:>7.4f} | \"\n","            f\"{f1_per_label[i]:>8.4f} | \"\n","            f\"{mcc_per_label[i]:>7.4f} | \"\n","            f\"{thresholds[i]:>9.4f}\"\n","        )\n","\n","    best_acc_idx = max(dim_acc, key=lambda x: dim_acc[x])\n","    best_f1_idx  = int(np.argmax(f1_per_label))\n","    best_mcc_idx = max(mcc_per_label, key=lambda x: mcc_per_label[x])\n","\n","    win_counts = {i: 0 for i in range(num_labels)}\n","    if dim_acc[best_acc_idx] > 0: win_counts[best_acc_idx] += 1\n","    if f1_per_label[best_f1_idx] > 0: win_counts[best_f1_idx] += 1\n","    if mcc_per_label[best_mcc_idx] > -1: win_counts[best_mcc_idx] += 1\n","\n","    if max(win_counts.values()) == 0:\n","         best_dim_idx = int(np.argmax(f1_per_label))\n","    else:\n","         best_dim_idx = max(win_counts, key=lambda x: win_counts[x])\n","\n","    best_dim_name = label_names[best_dim_idx]\n","\n","    best_acc_val = dim_acc[best_dim_idx]\n","    best_f1_val  = f1_per_label[best_dim_idx]\n","    best_mcc_val = mcc_per_label[best_dim_idx]\n","    best_thresh_val = thresholds[best_dim_idx]\n","\n","\n","    print(\n","        f\"\\nDimensi terbaik diprediksi (berdasarkan voting Accuracy, F1, MCC): \"\n","        f\"( {best_dim_name} ) \"\n","        f\"(Akurasi = {best_acc_val:.4f}) \"\n","        f\"(F1 = {best_f1_val:.4f}) \"\n","        f\"(MCC = {best_mcc_val:.4f}) \"\n","        f\"(Threshold = {best_thresh_val:.4f})\"\n","    )\n","\n","    metrics = {\n","        'overall_micro_acc': overall_micro_acc,\n","        'precision_micro': precision_micro,\n","        'recall_micro': recall_micro,\n","        'f1_micro': f1_micro,\n","        'mcc_overall': mcc_overall,\n","        'accuracy_per_dim': {label_names[i]: dim_acc[i] for i in range(num_labels)},\n","        'precision_per_dim': {label_names[i]: precision_per_label[i] for i in range(num_labels)},\n","        'recall_per_dim'   : {label_names[i]: recall_per_label[i] for i in range(num_labels)},\n","        'f1_per_dim'       : {label_names[i]: f1_per_label[i] for i in range(num_labels)},\n","        'mcc_per_dim'      : {label_names[i]: mcc_per_label[i] for i in range(num_labels)},\n","        'optimal_thresholds': {label_names[i]: thresholds[i] for i in range(num_labels)},\n","        'best_dim_by_vote': {\n","            'name': best_dim_name,\n","            'accuracy': best_acc_val,\n","            'f1': best_f1_val,\n","            'mcc': best_mcc_val,\n","            'threshold': best_thresh_val,\n","            'votes': win_counts[best_dim_idx]\n","        }\n","    }\n","    return metrics\n","\n","label_names = ['O', 'C', 'E', 'A', 'N']\n","test_metrics = test_and_evaluate_per_dimension(test_loader, model, Config.DEVICE, val_thresholds, label_names)"],"metadata":{"id":"5YvU1H7FaySK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["simpan metadata"],"metadata":{"id":"x2dfhMAlFdCw"}},{"cell_type":"code","source":["import json\n","import pandas as pd\n","from google.colab import files\n","import os\n","import datetime\n","\n","metadata = {\n","    'dataset_info': {\n","        'total_data': len(df),\n","        'train_size': len(train_dataset),\n","        'val_size':   len(val_dataset),\n","        'test_size':  len(test_dataset)\n","    },\n","    'hyperparameters': {\n","        'learning_rate': checkpoint['hyperparameters'].get('learning_rate'),\n","        'batch_size'   : checkpoint['hyperparameters'].get('batch_size'),\n","        'epochs'       : checkpoint['hyperparameters'].get('epochs'),\n","        'optimizer'    : checkpoint['hyperparameters'].get('optimizer'),\n","        'loss_fn'      : checkpoint['hyperparameters'].get('loss_fn'),\n","        'pin_memory'   : checkpoint['hyperparameters'].get('pin_memory'),\n","        'num_workers'  : checkpoint['hyperparameters'].get('num_workers'),\n","        'max_length'   : ConfigTM.MAX_LENGTH,\n","        'freeze_bert'  : True,\n","        'pretrained_model_name': 'distilbert-base-uncased',\n","        'hidden_dim'   : ConfigTM.HIDDEN_DIM,\n","        'dropout'      : 0.3,\n","        'num_labels'   : 5\n","    },\n","    'train_losses': checkpoint['train_losses'],\n","    'val_losses':   checkpoint['val_losses'],\n","    'val_thresholds': val_thresholds,\n","    'test_metrics': test_metrics,\n","    'training_time': checkpoint['training_time']\n","}\n","\n","base_dir = os.path.dirname(Config.CHECKPOINT_PATH)\n","timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n","\n","base_filename = f\"distilbert_metadata_thres_tuned_Gabungan Seluruh Dataset_biner([{metadata['hyperparameters']['learning_rate']}]-{metadata['hyperparameters']['batch_size']}-{metadata['hyperparameters']['epochs']}-{metadata['hyperparameters']['num_workers']})MX{metadata['hyperparameters']['max_length']} H{metadata['hyperparameters']['hidden_dim']} JMLH{metadata['dataset_info']['total_data']}\"\n","\n","metadata_path = os.path.join(base_dir, f'{base_filename}_{timestamp}.json')\n","\n","try:\n","    with open(metadata_path, 'w') as f:\n","        json.dump(metadata, f, indent=2)\n","    print(f\"Metadata saved to {metadata_path}\")\n","except Exception as e:\n","     print(f\"Error saving metadata file: {e}\")\n","\n","try:\n","    files.download(metadata_path)\n","except Exception as e:\n","    print(f\"Could not automatically download the file. Please download it manually from: {metadata_path}\")"],"metadata":{"id":"c1Ioln-sA74G"},"execution_count":null,"outputs":[]}]}